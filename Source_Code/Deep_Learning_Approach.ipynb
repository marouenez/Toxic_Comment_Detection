{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b6103d10-10b9-44dd-8c7c-ce71bb390833",
    "_uuid": "908cc6a5d46bae29f822e0dd353fd34947898706"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "05c286a1-2510-4c38-8922-6f4d8dcad863",
    "_uuid": "bead9ea1dfe1846027bfcd94a264a110b6f06e81"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "7e8ad169-36d2-4f13-8a6d-0b829451d6be",
    "_uuid": "a410746ad2dd36a2b9719382e04431eee5d34c68"
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"]\n",
    "list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "3d2dad9a-7ce2-42a7-be74-c737618294aa",
    "_uuid": "a46ed61c7a46e6201d070531ab1794e998268c0e"
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "7d7a8d69-2ade-472b-8c6b-fdb6a916f24c",
    "_uuid": "b9e0c623fb56fa637df7a9f0659d36f9c639ac51"
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "c884b17c-3365-4f05-bad3-b90928e07719",
    "_uuid": "fc2ad52bb6b418694dca15370c6b9bc47b2e304b"
   },
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(typeToLoad):\n",
    "    if(typeToLoad==\"glove\"):\n",
    "        EMBEDDING_FILE='./input/glove.6B.50d.txt'\n",
    "        embed_size = 25\n",
    "    elif(typeToLoad==\"word2vec\"):\n",
    "        word2vecDict = word2vec.KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "        embed_size = 300\n",
    "    elif(typeToLoad==\"fasttext\"):\n",
    "        EMBEDDING_FILE='./input/crawl-300d-2M.vec'\n",
    "        embed_size = 300\n",
    "\n",
    "    if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\n",
    "        embeddings_index = dict()\n",
    "            #Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "        f = open(EMBEDDING_FILE)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "                #first index is word\n",
    "            word = values[0]\n",
    "                #store the rest of the values in the array as a new array\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "            except ValueError:\n",
    "                pass\n",
    "            embeddings_index[word] = coefs #50 dimensions\n",
    "        f.close()\n",
    "        print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    else:\n",
    "        embeddings_index = dict()\n",
    "        for word in word2vecDict.wv.vocab:\n",
    "            embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "        print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            \n",
    "    gc.collect()\n",
    "        #We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "        #same statistics for the rest of our own random generated weights. \n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "        \n",
    "    nb_words = len(tokenizer.word_index)\n",
    "        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "        #the size will be Number of Words in Vocab X Embedding Size\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    gc.collect()\n",
    "\n",
    "        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "        #our own dictionary and loaded pretrained embedding. \n",
    "    embeddedCount = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        i-=1\n",
    "            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "            #and store inside the embedding matrix that we will train later on.\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embeddedCount+=1\n",
    "    print('total embedded:',embeddedCount,'common words')\n",
    "        \n",
    "    del(embeddings_index)\n",
    "    gc.collect()\n",
    "        \n",
    "        #finally, return the embedding matrix\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df4161dc-6900-4be0-ab99-0dd0c79afa1c",
    "_uuid": "b4981dc087c111871b303867491fc7480316c6f2"
   },
   "source": [
    "The function would return a new embedding matrix that has the loaded weights from the pretrained embeddings for the common words we have, and randomly initialized numbers that has the same mean and standard deviation for the rest of the weights in this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "77c36d68-86d4-4b7f-8099-c8acad87d116",
    "_uuid": "5482edd9abbcaf24e136a2c3b50f5d350cc01526"
   },
   "source": [
    "Let's move on and load our first embeddings from Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "dea6a774-9c3d-4b20-be59-225c5cb789de",
    "_uuid": "71c5959cf97fec554a9a44809c3947617915f4a1"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def embed_vec(EMBEDDING_FILE,embed_size, max_features = 20000,maxlen = 100):\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    print(embedding_matrix.shape)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "a117696b-2428-4692-baff-cc2adc2e2793",
    "_uuid": "8709b486e16f2f8d622afdcc3715025bd69e785e"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE_glove = './input/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "1dd12127-6b2a-4271-93a8-dda4db6cd405",
    "_uuid": "f6c844064e92b630a87d5389ab1d45ba3bd59d11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_glove = embed_vec(EMBEDDING_FILE_glove,embed_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "5b7702c7-3e85-4915-b809-8c3020ef8989",
    "_uuid": "8faa83f63a008bf6b0a151f854eeac23ab19410d"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Merge, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional \n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "8519fc8b-c2e0-48e8-b0d4-2a427dd6c53f",
    "_uuid": "e0d35cc890edb872c9b0576cb3f6ef51daa3fe36"
   },
   "outputs": [],
   "source": [
    "def cnn_model(embedding_matrix):\n",
    "    embedding_layer = Embedding(max_features, \n",
    "                                embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "\n",
    "    sequence_input = Input(shape=(maxlen,))\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = BatchNormalization()(Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences))\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = BatchNormalization()(Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences))\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    #x = Dropout(0.5)(pool)\n",
    "    x = Dropout(0.5)(l_merge) \n",
    "    x = Flatten()(x)\n",
    "    x = BatchNormalization()(Dense(128, activation='relu')(x))\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    preds = Dense(len(list(list_classes)), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "8076ef9c-fb8c-4a6d-8e1a-3163e350ffd3",
    "_uuid": "fe20bdce08d5c1d38f3241956d4d1abc442c34ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mintz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model = cnn_model(embedding_matrix_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "1ce978e1-ec45-4ce4-aa25-56b8d26074eb",
    "_uuid": "2a5feae023ffbca09f42e849aa1f725440b59c7d"
   },
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "4dbd724d-e546-417d-b37b-242ea24a362f",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "c7a3cff7916990deaa38228dab5e1edb441886ec",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/3\n",
      "127656/127656 [==============================] - 269s 2ms/step - loss: 0.1134 - acc: 0.9583 - val_loss: 0.0711 - val_acc: 0.9759\n",
      "Epoch 2/3\n",
      "127656/127656 [==============================] - 280s 2ms/step - loss: 0.0701 - acc: 0.9758 - val_loss: 0.0651 - val_acc: 0.9772\n",
      "Epoch 3/3\n",
      "127656/127656 [==============================] - 281s 2ms/step - loss: 0.0665 - acc: 0.9767 - val_loss: 0.0622 - val_acc: 0.9787\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "hist = model.fit(X_t, y, \n",
    "                 epochs=num_epochs, \n",
    "                 callbacks=callbacks_list, \n",
    "                 validation_split=0.2, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "162f04e3-1c08-499a-a7bd-faaf32e209c5",
    "_uuid": "fe57c166d02faaa4486e3e75b572563cb6cd7db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 94s 611us/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict(X_te, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f4b16653-059d-4de9-aa5a-b596e9dcbb1d",
    "_uuid": "d394d9e77f67e716e4a92b13fab6518830e4ca33"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"cnn_model_glove.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
